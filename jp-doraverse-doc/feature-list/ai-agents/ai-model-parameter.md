---
icon: align-center
---

# AIモデルパラメータ

## Temperature（温度）

### 目的

モデルの応答におけるランダム性の度合いを制御します。値を高くすると創造的で多様な出力になり、低くすると決定論的で予測可能な応答になります。

### 仕組み

温度が高い（例：0.9）場合、モデルはあまり一般的でない単語の組み合わせも積極的に選択します。温度が低い（例：0.2）場合、最も確率の高いトークンを優先し、より保守的でストレートな回答になります。

### ヒント

* **高い温度（例：0.8〜1.0）を設定**：ブレインストーミングや詩の生成、バリエーションを重視したい創造的なコンテンツに適しています。
* **低い温度（例：0.1〜0.3）を設定**：正確さが求められるタスクや、要約など構造化されたコンテンツ生成に適しています。

***

## **Top\_p**

### **目的**

モデルの出力の多様性を、各応答で考慮されるトークン確率の範囲を調整することで制御します。

### **仕組み**

「nucleus sampling（核サンプリング）」とも呼ばれ、top\_pは累積確率のしきい値を設定します。

例えばtop\_p=0.9の場合、合計確率が90%になる最小の単語グループからトークンが選ばれます。top\_pを下げると、より確率の高いトークンだけが使われるため、バリエーションが減ります。

### **ヒント**

* **高いtop\_p（0.9〜1.0）を設定**：自然な流れやバリエーションが重要な創造的・会話的タスクに適しています。
* **低いtop\_p（0.1〜0.3）を設定**：一貫性や集中が求められる技術的な指示生成などに適しています。

**注意**：**temperature**と**top\_p**の両方を設定した場合、両方の条件でトークンがサンプリングされます。通常はどちらか一方のみを調整するのが推奨されます。

***

## Max Tokens（最大トークン数）

### **目的**

モデルの応答内で使用できるトークン（単語、句読点、単語の一部など）の上限を設定します。英語の場合、1トークンはおおよそ4文字または3/4語に相当します。

### **仕組み**

このパラメータにより、応答が指定した長さを超えないように制御できます。短い上限は簡潔な出力に、長い上限は詳細な応答に適しています。

### **ヒント**

* **短い最大トークン数（10〜50）を設定**：タグラインや見出し、1文だけの出力に適しています。
* **長い最大トークン数（100〜500）を設定**：詳細な説明や長文コンテンツ、要約などに適しています。

***

## 繰り返しペナルティ

### **目的**

応答内で同じ単語やフレーズが繰り返されるのを抑制し、より多様な表現を促します。

### **仕組み**

**繰り返しペナルティ**を高く設定すると、同じトークンの再利用が減り、より魅力的でダイナミックな応答になります。

### **ヒント**

* **高い繰り返しペナルティ（0.5〜2.0）を設定**：表現のバリエーションが欲しい場合や、ストーリーやマーケティングコピーなど創造的なコンテンツ生成に適しています。
* **低い繰り返しペナルティ 0.0〜0.3）を設定**：繰り返しが許容される、または強調したい場合に適しています。

***

## **Presence Penalty**

### **目的**

応答内で新しいアイデアやトピックの導入を促進または抑制します。値を高くすると、モデルが新しい概念を探求しやすくなります。

### **仕組み**

このパラメータを上げると、モデルは既知のトピックやフレーズにとどまらず、より探索的・革新的な応答を生成します。

### **ヒント**

* **高いPresence Penalty（0.5〜2.0）を設定**：ブレインストーミングや新規性・多様性を重視する創造的なタスクに適しています。
* **低いPresence Penalty（0.0〜0.3）を設定**：特定のトピックに集中したい場合や、中心となるアイデアを強調したい場合に適しています。

***

## コンテキストトークンの上限

### **目的**

1回のリクエストで処理できるトークン数（入力プロンプト＋応答）の上限を定義します。

### **仕組み**

この値はモデルの「コンテキストウィンドウ」を表し、1回のやり取りで「記憶」または参照できるトークンの合計数です。

例：

* GPT-4.1：最大**1,047,576トークン**
* Gemini 2.5 Pro：最大**100万トークン**

この範囲内で以下が利用されます：

* 入力プロンプト＋チャット履歴
* 関数呼び出し定義
* システム指示
* モデルの応答

この上限を超えると、APIはエラーを返すか、コンテキストが切り捨てられます。

### **ヒント**

* このパラメータは手動で設定する必要はなく、システム側で管理されます。
* 長い会話やドキュメントを扱う開発者向けに有用です。
* 長文タスク（例：ドキュメントQ\&Aや要約）では、入力＋出力がトークン上限内に収まるようにしてください。

***

## アウトプットトークンの上限

### **目的**

モデルの応答の最大長（トークン数）を指定します。

### **仕組み**

このパラメータは、モデルがどこまでテキストを生成できるかの上限を設定します。1トークンは英語で約4文字、または約3/4語に相当します。

上限に達すると、モデルは**即座に出力を停止**します（文の途中でも止まります）。

### **ヒント**

* 短い（10〜50）：タグライン、箇条書き、簡単な要約など
* 中程度（100〜300）：段落やQ\&Aなどバランスの良い応答
* 長い（500以上）：ストーリー、エッセイ、詳細な説明など
* 詳細が足りない場合は、値を増やしてください。

***

## Sequenceを停止する

**目的**

モデルにテキスト生成を停止させる文字列（Sequence）を1つ以上定義します。

**仕組み**

指定したSequenceが生成された時点で、モデルは即座に出力を停止します（最大トークン数に達していなくても止まります）。

よく使われる例：

* `"\n\n"`（ダブル改行）
* "User:"（次のプロンプトで停止）
* "###"（セクション区切り用）

**ヒント**

* 出力のフォーマット制御やリスト終了、不要な出力防止に活用できます。
* 複数のSequenceを個別の文字列として指定可能です。
* 通常は上級者や開発者向けの設定です。

***

## **Reasoning Effort**

### **目的**

Reasoning Effortは、モデルが応答を生成する際にどれだけ深く「考える」かを制御します。値を高くすると、より慎重で熟慮された回答が得られやすくなります。

### **仕組み**

このパラメータはOpenAIのoシリーズモデル専用です。

値を高く設定すると応答速度がやや遅くなる場合がありますが、論理や説明が求められるタスクでは品質が向上することが多いです。Reasoning Effortを下げると、応答が速くなり、推論部分のトークン消費も抑えられます。

### **ヒント**

* 一般的な質問や素早い応答には**低〜中**を推奨します。
* 以下の場合は高に設定してください：
  * ステップごとの説明
  * 数学やコードの推論
  * 分析タスクや構造化された文章

***

## **Image Detail**

### **目的**

画像の詳細は、画像入力をどの程度詳細に解析するかを制御します。

### **仕組み**

* `低`：画像内容を素早く簡易にスキャンします。
* `高`：図表やドキュメント、詳細な画像の深い解析に適しています。
* `自動`：画像の種類や質問内容に応じて最適なレベルを自動選択します。

### **ヒント**

* スクリーンショットやUIボタンなどには「低」がおすすめです。
* 表や図、PDFなど密度の高い内容には「高」を選択してください。
* 通常は「自動」でバランスよく解析できます。

***

## ファイルを再送する

ファイルを再送するは、以前にアップロードしたファイルを再生成や再実行時に再処理・再送信するかどうかを切り替えるパラメータです。

### ヒント

* ファイル内容に依存するリクエスト（例：「このPDFを要約」）の場合はONのままにしてください。
* プロンプトの文言だけを調整したい場合や、遅延を避けたい場合はOFFにしてください。

***

## パラメータ調整のベストプラクティス <a href="#heading-best-practices-for-parameter-adjustment" id="heading-best-practices-for-parameter-adjustment"></a>

**試行錯誤**：さまざまな設定を試して、応答への影響を確認しましょう。

**バランス**：創造性と一貫性のバランスを意識してください。極端な値は意味の薄い出力につながる場合があります。

**コンテキスト意識**：利用シーンや目的に応じて最適な設定を選びましょう。状況によって最適なパラメータは異なります。
